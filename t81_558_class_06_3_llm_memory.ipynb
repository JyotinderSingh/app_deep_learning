{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Z2JH6cR3bM"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_06_3_alpaca_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On4gWCWuR3bN"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 6: ChatGPT and Large Language Models**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QVOz9xNR3bO"
      },
      "source": [
        "# Module 6 Material\n",
        "\n",
        "* Part 6.1: Introduction to Transformers [[Video]](https://www.youtube.com/watch?v=mn6r5PYJcu0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_1_transformers.ipynb)\n",
        "* Part 6.2: Accessing the ChatGPT API [[Video]](https://www.youtube.com/watch?v=tcdscXl4o5w&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_2_chat_gpt.ipynb)\n",
        "* **Part 6.3: LLM Memory** [[Video]](https://www.youtube.com/watch?v=oGQ3TQx1Qs8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_3_llm_memory.ipynb)\n",
        "* Part 6.4: Introduction to Embeddings [[Video]](https://www.youtube.com/watch?v=e6kcs9Uj_ps&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_4_embedding.ipynb)\n",
        "* Part 6.5: Prompt Engineering [[Video]](https://www.youtube.com/watch?v=miTpIDR7k6c&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_5_prompt_engineering.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLI2X9tnR3bO"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
        "  Running the following code will map your GDrive to ```/content/drive```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjyDLCCuR3bO",
        "outputId": "f2259d55-ece7-4ae3-95fc-d40e4aef987e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpAB-sNR3bP"
      },
      "source": [
        "# Part 6.3: LLM Memory\n",
        "\n",
        "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
        "\n",
        "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is Jeff\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
        "\n",
        "These memory objects, which LangChain provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
        "\n",
        "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation.\n",
        "\n",
        "## Install LangChain\n",
        "\n",
        "Just as we did previously, we mustinstall LangChain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_pz8MuY84Qh",
        "outputId": "d974c9a7-c5c9-44e3-ca0e-7563a2e39670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.0.2.post1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.10)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.7.2)\n",
            "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.5.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.6.0,>=0.5.2->langchain_openai) (2023.6.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain_openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE0FlZ_eR3bP"
      },
      "source": [
        "Smiliarly, we must also specify our API key and model touse. Refer to Module 6.2 for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VucO3HSMoJkz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "# Your OpenAI API key\n",
        "# If you are in my class at WUSTL, get this key from the Assignment 6 description in Canvas.\n",
        "\n",
        "OPENAI_KEY = '[insert your token]'\n",
        "\n",
        "# This is the model you will generally use for this class\n",
        "LLM_MODEL = 'gpt-3.5-turbo-instruct'\n",
        "\n",
        "# Chat Model: LLM_MODEL = 'gpt-3.5-turbo-1106'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create the LLM model, as we did before. We specify a higher temperature to encourage creativity in the conversation."
      ],
      "metadata": {
        "id": "W0McrknBXgxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
        "llm = OpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0.7)"
      ],
      "metadata": {
        "id": "D0CRDXlUW0V5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Buffer Window Memory"
      ],
      "metadata": {
        "id": "-6kkALepWLi9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aufxkOj1R3bP"
      },
      "source": [
        "The LangChain library includes a conversation object named **ConversationChain**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this first example, we will use the **ConversationBufferWindowMemory** object. This object keeps a transcript of the most recent conversation to reference. This memory allows the conversation object to remember what you have asked or told it and its responses to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w3_lzwcqermJ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxg_SXT2R3bP"
      },
      "source": [
        "We can now have a conversation with the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Egh3beCVRpW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "79c0b612-30ba-4f33-c9f6-49e86434cfe5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Jeff! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and provide helpful information. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Jeff\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gWi1o0tuYOR3",
        "outputId": "904552e9-3fe4-47df-9837-4160c4303c6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Your name is Jeff. I know this because you introduced yourself to me at the beginning of our conversation. Did you forget?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "W3u8kB-ccx2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC8dFPqoaa7Z",
        "outputId": "b8b4d49c-eda6-43ad-ffca-5ef443ba88d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Jeff\\nAI:  Hello Jeff! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and provide helpful information. How can I assist you today?\\nHuman: What is my name?\\nAI:  Your name is Jeff. I know this because you introduced yourself to me at the beginning of our conversation. Did you forget?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Conversation Bots\n",
        "\n",
        "You can define the prompt template for a conversationbot. This technique allows you to create a bot with a name and perform a specialized task. In this case, we created a bot named \"WashU Assistant\" that we designed to help students."
      ],
      "metadata": {
        "id": "9EJfbbLSYTnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can override it and set it to \"AI Assistant\"\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"The following is a friendly conversation between a human and an\n",
        "AI to assist Washington University Students. The AI should stick to topics\n",
        "about Washington University. If the AI does not know the answer to a question,\n",
        "it should suggest the student speak to their advisor.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "WashU Assistant:\"\"\"\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    memory=ConversationBufferWindowMemory(ai_prefix=\"WashU Assistant\"),\n",
        ")"
      ],
      "metadata": {
        "id": "3BZAlBkBYajX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now have a conversation with the WUSTL assistant bot."
      ],
      "metadata": {
        "id": "S7_y05_Pc8Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Where is the bookstore?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JBH_Zp0NYrW1",
        "outputId": "f8ad5eed-3f18-4adf-c3fa-d53a62e0732d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The bookstore is located on the South 40 residential area, next to the Danforth University Center. Do you need assistance with directions or any other information about the bookstore?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is a nice quiet area to study?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4LNHTxdMYu1h",
        "outputId": "1d09a0d9-8e7b-446f-b4a4-a78a6605b036"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' There are several quiet study areas on campus, including the Olin Library, the Art & Architecture Library, and the study rooms in the residential colleges. Is there a specific location or type of environment you prefer for studying?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Which of these is closest to the bookstore?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "my3IvjLhY1eh",
        "outputId": "d23cb1b3-6539-4a11-b097-ea9a04d2f776"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The Olin Library is the closest quiet study area to the bookstore. It is located right next to the Danforth University Center. Would you like me to provide you with directions?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is the meaning of life.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "-tx7V8d8Zlng",
        "outputId": "15f50507-66d2-4cfe-8901-a757f59adbfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The meaning of life is a question that has been pondered by humanity for centuries. While I am not equipped to provide a definitive answer, I can suggest exploring courses in philosophy or religion at Washington University to further explore this topic. Alternatively, you may want to speak with your academic advisor for guidance on courses that align with your interests.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "z3kzuQqRc2Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwq-I9lYaZuP",
        "outputId": "c3f8c699-d0fa-4e2a-c403-aa806d617d03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Jeff\\nAI:  Hello Jeff! It's nice to meet you. My name is AI, short for Artificial Intelligence. I am a computer program designed to simulate human conversation and provide helpful information. How can I assist you today?\\nHuman: What is my name?\\nAI:  Your name is Jeff. I know this because you introduced yourself to me at the beginning of our conversation. Did you forget?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Summary Memory\n",
        "\n",
        "Now, let's look at using a slightly more complex type of memory, the ConversationSummaryMemory object. This type of memory creates a summary of the conversation over time. This memory can be helpful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation and stores the current summary in memory. You can use this memory to inject the conversation summary so far into a prompt/chain. This memory is most useful for more extended conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "T6Yjeri4Zv9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "6wJsN0k_aMVS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I am a computer scientist, what do you do for a living?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4DEl67mQaPvp",
        "outputId": "9dfbcc70-186c-4646-9500-194591db6424"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I am an AI designed and developed by a team of computer scientists. My purpose is to assist and provide information to users like yourself. I am constantly learning and expanding my knowledge base through various data sources and algorithms.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a look at what the memory now contains."
      ],
      "metadata": {
        "id": "HG5LSIvgc4yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTFOPjHjaSBL",
        "outputId": "0a98bee2-6284-439a-8d14-622513191ef4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"\\nThe human inquires about the AI's occupation and the AI explains that it was created by a team of computer scientists to assist and provide information to users. The AI also mentions that it is continuously learning and expanding its knowledge base through data sources and algorithms.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.11 (torch)",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}